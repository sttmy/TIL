{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c06e99dc436454b84b402d186ebadb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tjoeun\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f037595d441043f59e99cf3b16670a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/5.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57796652c947443cb0d970c71a17b319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4fb72ae5eb43a7b84e2f7917de77a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989156c678c048988d06ef04e64cf98b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "890e4e544e2c4a9db510713965f97f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GPT-Neo : GPT의 오픈소스 버전\n",
    "\n",
    "# pip install transformers sentencepiece\n",
    "\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "\n",
    "# 모델 및 토크나이저 불러오기\n",
    "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   40, 16726,   262,  2854,   286,   402, 11571,    12,  8199,    78,\n",
      "         4166,   416,  4946, 20185,    13])\n",
      "I evaluated the performance of GPT-Neo developed by OpenAI.\n"
     ]
    }
   ],
   "source": [
    "# 텍스트=>정수 인코딩(1개의 문장)\n",
    "\n",
    "input = tokenizer.encode(\"I evaluated the performance of GPT-Neo developed by OpenAI.\", return_tensors=\"pt\")    #pt Pytorch, tf Tensorflow\n",
    "print(input[0])\n",
    "print(tokenizer.decode(input[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   40, 16726,   262,  2854,   286,   402, 11571,    12,  8199,    78,\n",
      "          4166,   416,  4946, 20185,    13],\n",
      "        [   40, 16726,   262,  2854,   286,   402, 11571,  4166,   416,  4946,\n",
      "         20185,    13, 50257, 50257, 50257]])\n",
      "['I evaluated the performance of GPT-Neo developed by OpenAI.', 'I evaluated the performance of GPT developed by OpenAI. [PAD] [PAD] [PAD]']\n"
     ]
    }
   ],
   "source": [
    "# 정수 인코딩(복수 문장)\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "input = tokenizer.batch_encode_plus(\n",
    "[\"I evaluated the performance of GPT-Neo developed by OpenAI.\",\n",
    " \"I evaluated the performance of GPT developed by OpenAI.\"], \n",
    " padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "print(input['input_ids'])\n",
    "print([tokenizer.decode(input['input_ids'][i]) for i in range(len(input['input_ids']))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 복수의 문장 인코딩\n",
    "\n",
    "input = tokenizer.batch_encode_plus(\n",
    "[\"I evaluated the performance of GPT2 developed by OpenAI.\", \"Vaccine for new coronavirus in the UK\", \"3.1415926535\"],\n",
    "max_length=5, truncation=True, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   40, 16726,   262,  2854,   286],\n",
       "        [   53,  4134,   500,   329,   649],\n",
       "        [   18,    13,  1415, 19707, 22980]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인코딩 결과 확인\n",
    "input['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "c:\\Python\\Python39\\lib\\site-packages\\transformers\\generation\\utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[   40, 16726,   262,  2854,   286,   262,  5150,  2446,   319,   262,\n",
       "          1103,    12,  6894, 27039,    13,   383,  2482,   389,  3402,   287],\n",
       "        [   53,  4134,   500,   329,   649,    12,  6286,    82,   198,   198,\n",
       "           464, 12319,   329,   649,    12,  6286,    82,   318,   257, 12319],\n",
       "        [   18,    13,  1415, 19707, 22980,  2327,  4531, 44750, 23721,  3510,\n",
       "         18897,  2091,  5999, 26050,  1120,  2078,  5705, 24991,  1433,  6052]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 만들기\n",
    "generated = model.generate(input['input_ids'])\n",
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. 1\n",
      "I evaluated the performance of the proposed method on the real-world dataset. The results are shown in\n",
      "\n",
      "No. 2\n",
      "Vaccine for new-borns\n",
      "\n",
      "The vaccine for new-borns is a vaccine\n",
      "\n",
      "No. 3\n",
      "3.1415926535897932384626433832795028841971693\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 생성된 문장 디코딩\n",
    "generated_text = tokenizer.batch_decode(generated)\n",
    "for i, sentence in enumerate(generated_text):\n",
    "    print(f'No. {i+1}')\n",
    "    print(f'{sentence}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf866dd75a641b8b40d3e5b14d01258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d32dcae1c142449215131d376c3cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8eda3af244e4cb0b69df4fadffb9f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836867ba2fc64914ad2f7da90cbd707b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b10a759d6c048e3a02b7a67c75d31f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4f77831fe24b5290b73c6bbaa4a8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DistilGPT2 모형 활용\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('distilgpt2')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I like gpt because it's a good thing to have\n"
     ]
    }
   ],
   "source": [
    "# DistilGPT2 모형으로 문장만들기\n",
    "input_ids = tokenizer.encode(\"I like gpt because it's\", return_tensors='pt')\n",
    "greedy_output = model.generate(input_ids, max_length=12)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I like gpt because it's a good thing to have\n"
     ]
    }
   ],
   "source": [
    "# DistilGPT2 모형으로 문장 만들기\n",
    "\n",
    "input_ids = tokenizer.encode(\"I like gpt because it's\", return_tensors='pt')\n",
    "\n",
    "greedy_output = model.generate(input_ids, max_length=12)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46cd9959d944403695d2c0deba9d11e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/351M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b67cd960fb45e5990451a2299ab68b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DialoGPT 모형 활용\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I like gpt because it's a good way to get a feel for the game.\n"
     ]
    }
   ],
   "source": [
    "#문장 만들기\n",
    "\n",
    "input_ids = tokenizer.encode(\"I like gpt because it's\", return_tensors='pt')\n",
    "greedy_output = model.generate(input_ids, max_length=30)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))\n",
    "\n",
    "#좀더 자연스러운 문장이 만들어짐 ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Covid19 delta is spreading the word\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"Covid19 delta is spreading\", return_tensors='pt')\n",
    "greedy_output = model.generate(input_ids, max_length=10)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add4f1efc5a14f9aa02c59467bd9509e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354d7a5e156345d09b7b89095b5549a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d367e7a49a421e85aab992c5602ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4911a9c91d0c4127921876f3b161a063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d869f6ea9ce241738a8e7be5d2774bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.1829017698764801,\n",
       "  'token': 2293,\n",
       "  'token_str': 'love',\n",
       "  'sequence': 'i love apple.'},\n",
       " {'score': 0.12623995542526245,\n",
       "  'token': 2066,\n",
       "  'token_str': 'like',\n",
       "  'sequence': 'i like apple.'},\n",
       " {'score': 0.11780297756195068,\n",
       "  'token': 2359,\n",
       "  'token_str': 'wanted',\n",
       "  'sequence': 'i wanted apple.'},\n",
       " {'score': 0.0684230700135231,\n",
       "  'token': 2215,\n",
       "  'token_str': 'want',\n",
       "  'sequence': 'i want apple.'},\n",
       " {'score': 0.05560746043920517,\n",
       "  'token': 3866,\n",
       "  'token_str': 'loved',\n",
       "  'sequence': 'i loved apple.'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLM(Masked Language Model) : 문서의 일부를 가리고(mask) 원래 단어를 추측하는 빈칸 채우기\n",
    "\n",
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "\n",
    "# [MASK]가 포함된 문장 입력\n",
    "unmasker(\"I [MASK] apple.\")\n",
    "\n",
    "# [MASK] 위치에 입력될 수 있는 단어들을 추천하여 문장 완성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.06269748508930206,\n",
       "  'token': 8823,\n",
       "  'token_str': 'ate',\n",
       "  'sequence': 'i ate apple.'},\n",
       " {'score': 0.0586443729698658,\n",
       "  'token': 2293,\n",
       "  'token_str': 'love',\n",
       "  'sequence': 'i love apple.'},\n",
       " {'score': 0.056702181696891785,\n",
       "  'token': 3866,\n",
       "  'token_str': 'loved',\n",
       "  'sequence': 'i loved apple.'},\n",
       " {'score': 0.051368582993745804,\n",
       "  'token': 6283,\n",
       "  'token_str': 'hated',\n",
       "  'sequence': 'i hated apple.'},\n",
       " {'score': 0.04913158714771271,\n",
       "  'token': 4521,\n",
       "  'token_str': 'eat',\n",
       "  'sequence': 'i eat apple.'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# distilbert 모델\n",
    "unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n",
    "unmasker(\"I [MASK] apple.\")\n",
    "\n",
    "# 모형에 따라 추천되는 단어가 달라짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ab1eb3ca724b0f85fd8e78f2f9706b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cee1764844d428e878aa5731fde115b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/47.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForMaskedLM: ['albert.pooler.bias', 'albert.pooler.weight']\n",
      "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4fd40e9e654fe58a2898fcc19a7201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e5303e711d4711a2a307e7b4686de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.1261998564004898,\n",
       "  'token': 339,\n",
       "  'token_str': 'love',\n",
       "  'sequence': 'i love apple.'},\n",
       " {'score': 0.092204749584198,\n",
       "  'token': 3345,\n",
       "  'token_str': 'liked',\n",
       "  'sequence': 'i liked apple.'},\n",
       " {'score': 0.056264329701662064,\n",
       "  'token': 2199,\n",
       "  'token_str': 'loved',\n",
       "  'sequence': 'i loved apple.'},\n",
       " {'score': 0.04441859945654869,\n",
       "  'token': 5285,\n",
       "  'token_str': 'hated',\n",
       "  'sequence': 'i hated apple.'},\n",
       " {'score': 0.03994071111083031,\n",
       "  'token': 3223,\n",
       "  'token_str': 'hate',\n",
       "  'sequence': 'i hate apple.'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# albert 모델 : bert의 정확도를 높이고 경량화한 모형\n",
    "unmasker = pipeline('fill-mask', model='albert-base-v2')\n",
    "unmasker(\"I [MASK] apple.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
